# -*- coding: utf-8 -*-
"""DS_PRAC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QCltiLysxAxvcuS6aUmKhZdECrFEGE92

**1) Data Wrangling 1**

Perform the following operations using Python on any open source dataset (e.g., data.csv)
1. Import all the required Python Libraries.
2. Locate open source data from the web (e.g., https://www.kaggle.com). Provide a clear 
description of the data and its source (i.e., URL of the web site).
3. Load the Dataset into pandas dataframe.
4. Data Preprocessing: check for missing values in the data using pandas isnull(), describe() 
function to get some initial statistics. Provide variable descriptions. Types of variables etc. 
Check the dimensions of the data frame.
5. Data Formatting and Data Normalization: Summarize the types of variables by checking the 
data types (i.e., character, numeric, integer, factor, and logical) of the variables in the data set. 
If variables are not in the correct data type, apply proper type conversions.
6. Turn categorical variables into quantitative variables in Python.
In addition to the codes and outputs, explain every operation that you do in the above steps and explain 
everything that you do to import/read/scrape the data set
"""

import pandas as pd
import numpy as np

# Downloading dataset from https://www.kaggle.com/c/titanic/data
df = pd.read_csv('/content/gender_submission.csv')
df.head()

missing_values = df.isnull().sum()
print(missing_values)

summary_stats = df.describe()
print(summary_stats)

column_names = df.columns
data_types = df.dtypes
print("Column Names:")
print(column_names)
print("Data Types:")
print(data_types)

dimensions = df.shape
print("rows, colomns")
print(dimensions)

variable_types = df.dtypes.value_counts()
print(variable_types)

df['PassengerId'] = df['PassengerId'].astype('category')
df['PassengerId']

df_encoded = pd.get_dummies(df)
df_encoded

"""**2) Data Wrangling II**

Create an “Academic performance” dataset of students and perform the following operations using 
Python.
1. Scan all variables for missing values and inconsistencies. If there are missing values and/or 
inconsistencies, use any of the suitable techniques to deal with them.
2. Scan all numeric variables for outliers. If there are outliers, use any of the suitable techniques 
to deal with them.
3. Apply data transformations on at least one of the variables. The purpose of this 
transformation should be one of the following reasons: to change the scale for better 
understanding of the variable, to convert a non-linear relation into a linear one, or to decrease 
the skewness and convert the distribution into a normal distribution.
Reason and document your approach properly.
"""

import pandas as pd
import numpy as np

# Create sample dataset
data = {
    'Student ID': [1, 2, 3, 4, 5],
    'Age': [18, 20, np.nan, 19, 22],
    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],
    'Exam Score': [90, 85, 75, 95, 65],
    'Study Hours': [4, 6, 8, np.nan, 5],
    'Attendance Percentage': [80, 95, 85, 70, 60]
}

df = pd.DataFrame(data)
df

# Check for missing values
print(df.isnull().sum())

# Output:
# Student ID                0
# Age                       1
# Gender                    0
# Exam Score                0
# Study Hours               1
# Attendance Percentage     0
# dtype: int64

# Inpute missing values with mean
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Study Hours'].fillna(df['Study Hours'].mean(), inplace=True)

# Check for inconsistencies
# no of time a value is given in dataset
print(df['Age'].value_counts())

# Detect outliers using IQR method
# Handle outliers by replacing with maximum and minimum values
lower_bound = 65
upper_bound = 95
df['Exam Score'] = np.where(df['Exam Score'] < lower_bound, lower_bound, df['Exam Score'])
df['Exam Score'] = np.where(df['Exam Score'] > upper_bound, upper_bound, df['Exam Score'])
df['Exam Score']

import matplotlib.pyplot as plt
import seaborn as sns

# Check the distribution of 'Attendance Percentage' variable
sns.histplot(df['Attendance Percentage'], kde=True)
plt.show()

# Apply square root transformation
df['Attendance Percentage'] = np.sqrt(df['Attendance Percentage'])

# Check the transformed distribution
sns.histplot(df['Attendance Percentage'], kde=True)
plt.show()

"""**3) Descriptive Statistics - Measures of Central Tendency and variability**

Perform the following operations on any open source dataset (e.g., data.csv)
1. Provide summary statistics (mean, median, minimum, maximum, standard deviation) for a dataset (age, income etc.) with numeric variables grouped by one of the qualitative 
(categorical) variable. For example, if your categorical variable is age groups and quantitative 
variable is income, then provide summary statistics of income grouped by the age groups. 
Create a list that contains a numeric value for each response to the categorical variable.
2. Write a Python program to display some basic statistical details like percentile, mean, 
standard deviation etc. of the species of ‘Iris-setosa’, ‘Iris-versicolor’ and ‘Iris-versicolor’ of 
iris.csv dataset.
Provide the codes with outputs and explain everything that you do in this step.

1st part
"""

# dataset from https://www.kaggle.com/code/bhattacharjeeajay12/insuarance/input

import pandas as pd

# Read the dataset
data = pd.read_csv('/content/insurance.csv')
data.head()

# Calculate summary statistics grouped by a categorical variable
categorical_variable = 'age'  # Replace with your actual column name
numeric_variable = 'charges'  # Replace with your actual column name

summary_stats = data.groupby(categorical_variable)[numeric_variable].describe()

# Extract the desired statistics for each category
desired_statistics = ['mean', 'median', 'min', 'max', 'std']

# Create a list of numeric values for each response to the categorical variable
category_values = summary_stats.index.tolist()

# Print the summary statistics
print(summary_stats)
print("Category values:", category_values)

"""2nd part"""

# Download dataset from https://www.kaggle.com/datasets/uciml/iris

# !pip install pandas numpy
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('/content/Iris.csv')
df.head()

# Filter dataset for 'Iris-setosa', 'Iris-versicolor', and 'Iris-virginica'
setosa_df = df[df['Species'] == 'Iris-setosa']
versicolor_df = df[df['Species'] == 'Iris-versicolor']
virginica_df = df[df['Species'] == 'Iris-virginica']

# Calculate statistical details for each species
setosa_stats = setosa_df.describe()
versicolor_stats = versicolor_df.describe()
virginica_stats = virginica_df.describe()

print("Statistical details for 'Iris-setosa':")
print(setosa_stats)

print("\nStatistical details for 'Iris-versicolor':")
print(versicolor_stats)

print("\nStatistical details for 'Iris-virginica':")
print(virginica_stats)

"""**4) Data Analytics I**

Create a Linear Regression Model using Python/R to predict home prices using Boston Housing 
Dataset (https://www.kaggle.com/c/boston-housing). The Boston Housing dataset contains 
information about various houses in Boston through different parameters. There are 506 samples and 
14 feature variables in this dataset.
The objective is to predict the value of prices of the house using the given features.
"""

# !pip install pandas numpy scikit-learn matplotlib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/content/submission_example.csv')
df.head()

# Split the dataset into input features and target variable
X = df.drop('medv', axis=1)
y = df['medv']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Calculate mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calculate coefficient of determination (R^2)
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print("Mean Squared Error (MSE):", mse)
print("Coefficient of Determination (R^2):", r2)

# Example: Predict home prices for new_data
new_data = pd.DataFrame([1])
predicted_prices = model.predict(new_data)
print("Predicted prices:", predicted_prices)

"""**5) Data Analytics II**

1. Implement logistic regression using Python/R to perform classification on 
Social_Network_Ads.csv dataset.
2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall
on the given dataset.
"""

# Download dataset from https://www.kaggle.com/datasets/rakeshrau/social-network-ads

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

# Load the dataset
data = pd.read_csv("/content/Social_Network_Ads.xls")
data.head()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix


# Separate the features (X) and the target variable (y)
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

# Perform label encoding on the 'Gender' column
le = LabelEncoder()
X[:, 1] = le.fit_transform(X[:, 1])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an instance of the Logistic Regression model
logistic_regression = LogisticRegression()

# Train the model on the training data
logistic_regression.fit(X_train, y_train)

# Predict the labels for the test set
y_pred = logistic_regression.predict(X_test)

# Compute the confusion matrix
confusion = confusion_matrix(y_test, y_pred)

# Extract the values from the confusion matrix
TN = confusion[0, 0]  # True Negative
FP = confusion[0, 1]  # False Positive
FN = confusion[1, 0]  # False Negative
TP = confusion[1, 1]  # True Positive

# Compute the accuracy
accuracy = (TP + TN) / (TP + TN + FP + FN)

# Compute the error rate
error_rate = (FP + FN) / (TP + TN + FP + FN)

# Compute the precision
precision = TP / (TP + FP)

# Compute the recall
recall = TP / (TP + FN)

# display the confusion matrix
print(confusion)

# display the accuracy
print(accuracy)

# display the error rate
print(error_rate)

# display the precision
print(precision)

# display the recall
print(recall)

"""**6) Data Analytics III**

1. Implement Simple Naïve Bayes classification algorithm using Python/R on iris.csv dataset.
2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall on 
the given dataset.
"""

# Download dataset from https://www.kaggle.com/datasets/uciml/iris

# !pip install pandas numpy sklearn matplotlib

# import libs
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

# Load the dataset
df = pd.read_csv('/content/Iris.csv')

# Split the dataset into features and labels
X = df.drop('Species', axis=1)
y = df['Species']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Gaussian Naive Bayes classifier
classifier = GaussianNB()

# Train the classifier
classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = classifier.predict(X_test)

# Compute the confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(confusion_mat)

# Extract TN, FP, FN, TP from the confusion matrix
tn, fp, fn, tp = confusion_mat[0, 0], confusion_mat[0, 1], confusion_mat[1, 0], confusion_mat[1, 1]

# Compute evaluation metrics
accuracy = (tp + tn) / (tp + tn + fp + fn)
error_rate = 1 - accuracy
precision = tp / (tp + fp)
recall = tp / (tp + fn)

# Print evaluation metrics
print("Accuracy:", accuracy)
print("Error Rate:", error_rate)
print("Precision:", precision)
print("Recall:", recall)

"""**7) Text Analytics**

1. Extract Sample document and apply following document preprocessing methods: 
Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.
2. Create representation of documents by calculating Term Frequency and Inverse 
DocumentFrequency.

Part 1
"""

document = """
Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans using natural language. It involves the analysis, understanding, and generation of human language, enabling machines to process and comprehend text in a meaningful way. NLP techniques are widely used in various applications such as sentiment analysis, machine translation, chatbots, and information retrieval. Preprocessing is an essential step in NLP, which involves tokenization, part-of-speech tagging, stop words removal, stemming, and lemmatization.
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('wordnet')




# Tokenization
tokens = word_tokenize(document)

# POS Tagging
pos_tags = pos_tag(tokens)

# Stop words removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [token for token in tokens if token.lower() not in stop_words]

# Stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]

# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]

# Print the results
print("Original Document:\n", document)
print("\nTokens:\n", tokens)
print("\nPOS Tags:\n", pos_tags)
print("\nFiltered Tokens (after stop words removal):\n", filtered_tokens)
print("\nStemmed Tokens:\n", stemmed_tokens)
print("\nLemmatized Tokens:\n", lemmatized_tokens)

"""Part 2"""

from sklearn.feature_extraction.text import TfidfVectorizer

# List of documents
documents = [
    "Natural language processing is a subfield of artificial intelligence.",
    "It focuses on the interaction between computers and humans using natural language.",
    "NLP techniques are widely used in various applications such as sentiment analysis and machine translation.",
    "Preprocessing is an essential step in NLP.",
]

# Create an instance of TfidfVectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the documents
tfidf_matrix = vectorizer.fit_transform(documents)

# Get the feature names (terms)
feature_names = vectorizer.get_feature_names_out()

# Print the TF-IDF representation
for i, doc in enumerate(documents):
    print(f"Document {i+1}:")
    for j, term in enumerate(feature_names):
        tfidf_value = tfidf_matrix[i, j]
        if tfidf_value > 0:
            print(f"{term}: {tfidf_value:.4f}")
    print()

"""**8) Data Visualization I**

1. Use the inbuilt dataset 'titanic'. The dataset contains 891 rows and contains information about 
the passengers who boarded the unfortunate Titanic ship. Use the Seaborn library to see if we 
can find any patterns in the data.
2. Write a code to check how the price of the ticket (column name: 'fare') for each passenger
is distributed by plotting a histogram.

Part 1
"""

import pandas as pd
import seaborn as sns

# Load the Titanic dataset
df = sns.load_dataset('titanic')

# Display the first few rows of the dataset
print(df.head())

# Countplot:
sns.countplot(x='survived', data=df)

# Barplot:
sns.barplot(x='sex', y='survived', hue='class', data=df)

# Heatmap:
# sns.heatmap(df.corr(), annot=True, cmap='coolwarm')

# Convert categorical variables using one-hot encoding
df_encoded = pd.get_dummies(df_numeric)

# Create correlation matrix and heatmap
sns.heatmap(df_encoded.corr(), annot=True, cmap='coolwarm')

"""Part 2"""

import seaborn as sns
import matplotlib.pyplot as plt

# Load the Titanic dataset
df = sns.load_dataset('titanic')

# Plot histogram of ticket prices
sns.histplot(data=df, x='fare', kde=True)

# Set plot title and labels
plt.title('Distribution of Ticket Prices')
plt.xlabel('Ticket Fare')
plt.ylabel('Count')

# Display the plot
plt.show()

"""**9) Data Visualization II**

1. Use the inbuilt dataset 'titanic' as used in the above problem. Plot a box plot for distribution of 
age with respect to each gender along with the information about whether they survived or 
not. (Column names : 'sex' and 'age')
2. Write observations on the inference from the above statistics.

Part 1
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the 'titanic' dataset
titanic_df = sns.load_dataset('titanic')

# Filter out the necessary columns
data = titanic_df[['sex', 'age', 'survived']]

# Remove rows with missing age values
data = data.dropna(subset=['age'])

# Create a box plot using seaborn
sns.boxplot(x='sex', y='age', hue='survived', data=data)

# Set the plot title and labels
plt.title('Distribution of Age with respect to Gender and Survival')
plt.xlabel('Gender')
plt.ylabel('Age')

# Show the plot
plt.show()

"""Part 2

From the box plot showing the distribution of age with respect to each gender and the information about survival, we can make several observations:

1. Median Age: The median age for both genders appears to be similar, with the boxes for males and females overlapping. It suggests that, on average, the age distribution is relatively similar between males and females.

2. Age Range: The box plots indicate that the age range for males is slightly wider than that of females. The whiskers of the box plot for males extend further, indicating a broader range of ages among males.

3. Outliers: There are some outliers present in both the male and female distributions, represented by individual data points beyond the whiskers of the box plots. These outliers suggest the presence of individuals with significantly higher ages compared to the rest of the group.

4. Survival Differences: By considering the hue of the box plot, we can observe the differences in survival outcomes for each gender. Within each gender group, the box plot is divided into two colors: one representing individuals who survived and the other representing those who did not survive. We can see that the distribution of age for survivors and non-survivors is relatively similar for both genders.

Overall, from this box plot, we can infer that age alone does not seem to be a significant factor in determining survival on the Titanic. The distributions of age for both genders are relatively similar, and there is no clear distinction between survivors and non-survivors based on age within each gender group. Other factors such as cabin class, location on the ship, and potentially gender itself may have played a more significant role in determining survival.

**10) Data Visualization III**

Download the Iris flower dataset or any other dataset into a DataFrame. (e.g., 
https://archive.ics.uci.edu/ml/datasets/Iris ). Scan the dataset and give the inference as:
1. List down the features and their types (e.g., numeric, nominal) available in the dataset.
2. Create a histogram for each feature in the dataset to illustrate the feature distributions.
3. Create a boxplot for each feature in the dataset.
4. Compare distributions and identify outliers.
"""

import pandas as pd
import matplotlib.pyplot as plt

# Download the Iris dataset from the provided URL
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
iris_df = pd.read_csv(url, header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])
iris_df.head()

# List down the features and their types
features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
feature_types = ['numeric', 'numeric', 'numeric', 'numeric']

# Print the features and their types
for feature, ftype in zip(features, feature_types):
    print(f"Feature: {feature} - Type: {ftype}")

# Create histograms for each feature
iris_df[features].hist()
plt.suptitle('Histograms of Iris Dataset Features')
plt.tight_layout()
plt.show()

# Create box plots for each feature
iris_df[features].plot(kind='box')
plt.title('Box Plots of Iris Dataset Features')
plt.show()

# Identify outliers
outliers = []
for feature in features:
    q1 = iris_df[feature].quantile(0.25)
    q3 = iris_df[feature].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    feature_outliers = iris_df[(iris_df[feature] < lower_bound) | (iris_df[feature] > upper_bound)]
    outliers.append(feature_outliers)

# Print the outliers for each feature
for feature, outlier_df in zip(features, outliers):
    print(f"\nOutliers for feature: {feature}")
    print(outlier_df)

"""**Impala Code**

1.Create databases and tables, insert small amounts of data, and run simple queries using Impala

*Note:  Run this code on SQL query engine that runs on Apache Hadoop*
"""

# Creating a Database:
CREATE DATABASE mydatabase;


# Switching to the Database:
USE mydatabase;


# Creating Tables:
CREATE TABLE employees (
  id INT,
  name STRING,
  age INT,
  department STRING
);


# Inserting Data:
INSERT INTO employees VALUES
  (1, 'John Doe', 30, 'Sales'),
  (2, 'Jane Smith', 35, 'Marketing'),
  (3, 'David Johnson', 28, 'Engineering');


# Running Queries:
-- Select all employees
SELECT * FROM employees;

-- Select employees in the Sales department
SELECT * FROM employees WHERE department = 'Sales';

-- Count the number of employees
SELECT COUNT(*) FROM employees;

-- Calculate the average age of employees
SELECT AVG(age) FROM employees;